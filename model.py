# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pVFs5bUGoaHGWaZcvlpBFlxGoa5ik_LD
"""

import pandas as pd

df = pd.read_csv('datasets_33080_43333_car data.csv')

df.head()

df.shape

print(df['Seller_Type'].unique())

print(df['Transmission'].unique())
print(df['Owner'].unique())

"""Check for missing values"""

df.isnull().sum()

df.describe()

print(df['Year'].unique())

"""Derive a feature called number of years from the year"""

df['Number of Years'] = 2020 - df['Year']

df.head()

df.columns

final_dataset = df[['Year', 'Selling_Price', 'Present_Price', 'Kms_Driven',
       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner', 'Number of Years']]

final_dataset.head()

final_dataset.drop(['Year'],axis=1,inplace=True)

final_dataset.head()

"""Converting categorical features by one hot encoding"""

final_dataset = pd.get_dummies(final_dataset,drop_first=True)

final_dataset.head()

import seaborn as sns

sns.heatmap(final_dataset.corr())

sns.pairplot(final_dataset)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

"""Plotting correlation using heatmap"""

corrmat = final_dataset.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(10,10))
g = sns.heatmap(final_dataset[top_corr_features].corr(),annot=True,cmap='RdYlGn')

top_corr_features

X = final_dataset.iloc[:,1:]
y = final_dataset.iloc[:,0]

X.head()

y.head()

### feature importance
from sklearn.ensemble import ExtraTreesRegressor

model = ExtraTreesRegressor()
model.fit(X,y)

print(model.feature_importances_)

feature_importance = pd.Series(model.feature_importances_,index=X.columns)
feature_importance.nlargest(5).plot(kind='barh')

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

X_train.shape

from sklearn.ensemble import RandomForestRegressor

rf_Random = RandomForestRegressor()

###Hyperparameters
import numpy as np
n_estimators = [int(x) for x in np.linspace(start=100,stop=1200,num=12)]
print(n_estimators)

## Randomized search cv

#n_estimators
n_estimators = [int(x) for x in np.linspace(start=100,stop=1200,num=12)]

#number of features to consider at every split
max_features = ['auto','sqrt']

#Maximum no.of levels in a tree
max_depth = [int(x) for x in np.linspace(start=5,stop=30, num=6)]

#Minimum num of samples required to a node
min_samples_split = [2, 5, 10, 15, 100]

#minimum no.of samples at each leaf node
min_samples_leaf = [1, 2, 5, 10]

from sklearn.model_selection import RandomizedSearchCV

#create the randomgrid
random_grid = {'n_estimators':n_estimators,
               'max_features':max_features,
               'max_depth':max_depth,
               'min_samples_split':min_samples_split,
               'min_samples_leaf':min_samples_leaf
               }
print(random_grid)

rf = RandomForestRegressor()

rf_random = RandomizedSearchCV(estimator=rf,param_distributions=random_grid,scoring='neg_mean_squared_error',n_iter=10,cv=5,verbose=2,random_state=42,n_jobs=1)

rf_random.fit(X_train,y_train)

rf_random.best_estimator_

predictions = rf_random.predict(X_test)

sns.distplot(y_test-predictions)

plt.scatter(y_test,predictions)

import pickle
#open a file where you want to store the data
file = open('random_forest_regression_model1.pkl','wb')

#dum information to the file
pickle.dump(rf_random, file)



